"""Evolved Surrogate Model for GEO Strategy Selection.

This module uses the trained Critic model from agentic_geo training
and the evolved strategies from the strategy archive.

Key differences from surrogate_model_qwen.py:
1. Uses evolved strategies (25 strategies) instead of 10 fixed strategies
2. Critic input uses SHORT_PROMPT (for evaluation)
3. Strategy application uses FULL_PROMPT (for Oracle Rewrite)
4. Content truncation uses head+tail mode (max_tokens=1400)
5. Loads Critic with value_head.bin + optional LoRA adapter
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from typing import List, Tuple, Dict, Optional, Any
import os
import json


def truncate_content(content: str, max_tokens: int = 1400, head_tail: bool = True) -> str:
    """Truncate content, Head+Tail mode supported.

    IMPORTANT: This must match the truncation used during training to ensure
    training-inference consistency.

    Args:
        content: Original content
        max_tokens: Max token count (approx chars/4)
        head_tail: Use Head+Tail mode

    Returns:
        Truncated content
    """
    if content is None:
        return ""

    max_chars = max_tokens * 4

    if len(content) <= max_chars:
        return content

    if head_tail:
        half = max_chars // 2 - 10
        return content[:half] + " ... " + content[-half:]
    else:
        return content[:max_chars]


def build_critic_prompt(
    query: str,
    content: str,
    strategy_prompt: str,
    max_content_tokens: int = 1400
) -> str:
    """Build Critic input prompt (aligned with training)."""
    content = truncate_content(content, max_content_tokens, head_tail=True)

    prompt = f"""# Scenario
A website source will be used together with other sources to answer the user query. The final answer is generated by an LLM and each line may be cited. The goal is to increase this source's visibility/citation without changing its core meaning.

# Query
{query}

# Original Content
{content}

# Optimization Strategy
{strategy_prompt}

Based on the content and query, predict the quality score of this strategy."""

    return prompt


class SurrogateValueHead(nn.Module):
    """Surrogate Model MLP regression head.

    Aligned with LLaMA-Factory surrogate pipeline:
    LayerNorm -> Linear(hidden, 1024) -> GELU -> Linear(1024, 1)
    """

    def __init__(
        self,
        hidden_size: int = 1536,
        intermediate_size: int = 1024,
        dropout: float = 0.0,
        init_bias: float = 2.1
    ):
        super().__init__()

        self.layer_norm = nn.LayerNorm(hidden_size)
        self.fc1 = nn.Linear(hidden_size, intermediate_size)
        self.activation = nn.GELU()
        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()
        self.fc2 = nn.Linear(intermediate_size, 1)

        self._init_weights(init_bias)

    def _init_weights(self, init_bias: float):
        nn.init.orthogonal_(self.fc1.weight)
        nn.init.zeros_(self.fc1.bias)
        nn.init.orthogonal_(self.fc2.weight)
        nn.init.constant_(self.fc2.bias, init_bias)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        Args:
            hidden_states: (batch_size, hidden_size)

        Returns:
            scores: (batch_size,)
        """
        x = self.layer_norm(hidden_states)
        x = self.fc1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x.squeeze(-1)


class EvolvedCriticModel:
    """
    Evolved Critic Model for strategy evaluation.

    Loads the trained Critic from agentic_geo training:
    - Base model structure: Qwen2.5-1.5B-Instruct
    - Pretrained backbone weights: from surrogate_full1 checkpoint (if provided)
    - LoRA adapter: trained adapter from agentic_geo
    - Value head: trained value_head.bin
    """

    def __init__(
        self,
        base_model_path: str,
        value_head_path: str,
        lora_adapter_path: Optional[str] = None,
        pretrained_backbone_path: Optional[str] = None,
        device: str = None,
        use_chat_template: bool = True,
        cutoff_len: int = 2048,
        torch_dtype: str = "bfloat16"
    ):
        """Initialize the evolved Critic model.

        Args:
            base_model_path: Path to Qwen2.5-1.5B-Instruct base model (for structure)
            value_head_path: Path to trained value_head.bin
            lora_adapter_path: Optional path to LoRA adapter directory
            pretrained_backbone_path: Optional path to pretrained backbone weights
                (e.g., surrogate_full1/stage1/checkpoint-610/exported_bin/pytorch_model.bin)
            device: Device to use ('cuda', 'cpu', or None for auto)
            use_chat_template: Whether to use chat template for input formatting
            cutoff_len: Maximum sequence length
            torch_dtype: Model precision ('float32', 'bfloat16', 'float16')
        """
        self.use_chat_template = use_chat_template
        self.cutoff_len = cutoff_len

        # Device selection
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = torch.device(device)

        print(f"[EvolvedCritic] Loading Critic model")
        print(f"[EvolvedCritic] Base model: {base_model_path}")
        print(f"[EvolvedCritic] Pretrained backbone: {pretrained_backbone_path}")
        print(f"[EvolvedCritic] Value head: {value_head_path}")
        print(f"[EvolvedCritic] LoRA adapter: {lora_adapter_path}")
        print(f"[EvolvedCritic] Device: {self.device}")
        print(f"[EvolvedCritic] Dtype: {torch_dtype}")

        # Parse dtype
        dtype_map = {
            "float32": torch.float32,
            "bfloat16": torch.bfloat16,
            "float16": torch.float16,
        }
        model_dtype = dtype_map.get(torch_dtype, torch.bfloat16)

        # Load tokenizer
        print(f"[EvolvedCritic] Loading tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            use_fast=True
        )

        # Load backbone
        print(f"[EvolvedCritic] Loading backbone structure from base model...")
        self.backbone = AutoModel.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            torch_dtype=model_dtype
        )

        # Load pretrained backbone weights if provided
        if pretrained_backbone_path and os.path.exists(pretrained_backbone_path):
            print(f"[EvolvedCritic] Loading pretrained backbone weights from {pretrained_backbone_path}...")
            self._load_backbone_checkpoint(pretrained_backbone_path, model_dtype)

        # Load LoRA adapter if provided
        if lora_adapter_path and os.path.exists(lora_adapter_path):
            try:
                from peft import PeftModel
                print(f"[EvolvedCritic] Loading LoRA adapter from {lora_adapter_path}...")
                self.backbone = PeftModel.from_pretrained(
                    self.backbone,
                    lora_adapter_path,
                    torch_dtype=model_dtype
                )
                print(f"[EvolvedCritic] LoRA adapter loaded successfully")
            except ImportError:
                print(f"[EvolvedCritic] Warning: peft not installed, skipping LoRA adapter")
            except Exception as e:
                print(f"[EvolvedCritic] Warning: Failed to load LoRA adapter: {e}")

        # Move backbone to device
        self.backbone = self.backbone.to(self.device)
        self.backbone.eval()

        # Disable KV cache for inference
        if hasattr(self.backbone, "config") and hasattr(self.backbone.config, "use_cache"):
            self.backbone.config.use_cache = False

        # Get hidden size
        hidden_size = self.backbone.config.hidden_size
        print(f"[EvolvedCritic] Backbone hidden size: {hidden_size}")

        # Create value head
        self.value_head = SurrogateValueHead(hidden_size=hidden_size)

        # Load value head weights
        if value_head_path and os.path.exists(value_head_path):
            print(f"[EvolvedCritic] Loading value head weights from {value_head_path}...")
            value_head_state = torch.load(value_head_path, map_location="cpu")

            # Handle different save formats
            if isinstance(value_head_state, dict):
                if "state_dict" in value_head_state:
                    value_head_state = value_head_state["state_dict"]
                # Remove "value_head." prefix if present
                new_state = {}
                for k, v in value_head_state.items():
                    if k.startswith("value_head."):
                        new_state[k[11:]] = v
                    else:
                        new_state[k] = v
                if new_state:
                    value_head_state = new_state

            # Load weights
            try:
                self.value_head.load_state_dict(value_head_state, strict=True)
                print(f"[EvolvedCritic] Value head weights loaded successfully")
            except Exception as e:
                print(f"[EvolvedCritic] Warning: Strict loading failed, trying non-strict: {e}")
                self.value_head.load_state_dict(value_head_state, strict=False)
        else:
            print(f"[EvolvedCritic] Warning: Value head path not found: {value_head_path}")

        # Move value head to device and convert to same dtype as backbone
        self.value_head = self.value_head.to(device=self.device, dtype=model_dtype)
        self.value_head.eval()

        # Store dtype for later use
        self.model_dtype = model_dtype

        print(f"[EvolvedCritic] Model ready for inference")

    def _load_backbone_checkpoint(self, ckpt_path: str, model_dtype):
        """Load pretrained backbone weights from checkpoint file.

        Args:
            ckpt_path: Path to pytorch_model.bin or similar checkpoint file
            model_dtype: Target dtype for the model
        """
        if not os.path.exists(ckpt_path):
            print(f"[EvolvedCritic] Warning: Backbone checkpoint not found: {ckpt_path}")
            return

        print(f"[EvolvedCritic] Loading backbone checkpoint from {ckpt_path}...")
        checkpoint = torch.load(ckpt_path, map_location="cpu")

        # Parse state_dict
        state_dict = None
        if isinstance(checkpoint, dict):
            if "state_dict" in checkpoint:
                state_dict = checkpoint["state_dict"]
            elif "model" in checkpoint:
                state_dict = checkpoint["model"]
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint

        # Filter out value_head weights (we'll load those separately)
        backbone_state = {}
        for key, value in state_dict.items():
            # Skip value_head weights
            if "value_head" in key or "score" in key:
                continue
            # Remove prefixes to match model's expected key format
            new_key = key
            # Handle "backbone.model.xxx" -> "xxx"
            if new_key.startswith("backbone.model."):
                new_key = new_key[15:]  # len("backbone.model.") = 15
            # Handle "backbone.xxx" -> "xxx" (if no model. after backbone.)
            elif new_key.startswith("backbone."):
                new_key = new_key[9:]  # len("backbone.") = 9
            # Handle "model.xxx" -> "xxx"
            elif new_key.startswith("model."):
                new_key = new_key[6:]  # len("model.") = 6
            backbone_state[new_key] = value

        if backbone_state:
            # Load weights
            missing, unexpected = self.backbone.load_state_dict(backbone_state, strict=False)
            if missing:
                print(f"[EvolvedCritic] Missing keys in backbone: {len(missing)} keys")
                if len(missing) <= 5:
                    print(f"[EvolvedCritic]   {missing}")
            if unexpected:
                print(f"[EvolvedCritic] Unexpected keys in backbone: {len(unexpected)} keys")
                if len(unexpected) <= 5:
                    print(f"[EvolvedCritic]   {unexpected}")
            print(f"[EvolvedCritic] Backbone weights loaded ({len(backbone_state)} tensors)")
        else:
            print(f"[EvolvedCritic] Warning: No backbone weights found in checkpoint")

    def score_strategy(
        self,
        query: str,
        context: str,
        strategy_prompt: str
    ) -> float:
        """Score a single strategy using SHORT_PROMPT.

        Args:
            query: User query
            context: Content to optimize
            strategy_prompt: SHORT strategy prompt (for Critic evaluation)

        Returns:
            Predicted reward score
        """
        # Build prompt (content will be truncated inside)
        prompt = build_critic_prompt(
            query=query,
            content=context,
            strategy_prompt=strategy_prompt,
            max_content_tokens=1400
        )

        # Apply chat template if needed
        if self.use_chat_template:
            messages = [{"role": "user", "content": prompt}]
            formatted = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
        else:
            formatted = prompt

        # Tokenize
        inputs = self.tokenizer(
            formatted,
            return_tensors="pt",
            truncation=True,
            max_length=self.cutoff_len,
            padding=True
        ).to(self.device)

        # Forward pass
        with torch.no_grad():
            outputs = self.backbone(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"]
            )

            # Get last token hidden state
            last_hidden = outputs.last_hidden_state
            seq_len = inputs["attention_mask"].sum(dim=1) - 1
            last_token_hidden = last_hidden[0, seq_len[0]]

            # Value head forward
            value = self.value_head(last_token_hidden.unsqueeze(0))

        return value.item()

    def rank_strategies(
        self,
        query: str,
        context: str,
        strategies: List[Dict[str, Any]]
    ) -> List[Tuple[str, float, Dict[str, Any]]]:
        """Rank all strategies for given query and context.

        Args:
            query: User query
            context: Content to optimize
            strategies: List of strategy dicts with 'genotype_id', 'short_prompt', 'full_prompt', etc.

        Returns:
            List of (genotype_id, reward, strategy_dict) sorted by reward (descending)
        """
        results = []

        for strategy in strategies:
            # Use SHORT_PROMPT for Critic evaluation
            short_prompt = strategy.get('short_prompt', '')
            reward = self.score_strategy(query, context, short_prompt)
            results.append((
                strategy.get('genotype_id', ''),
                reward,
                strategy
            ))

        # Sort by reward (descending)
        results.sort(key=lambda x: x[1], reverse=True)
        return results


class EvolvedStrategySelector:
    """Strategy selector using evolved strategies and trained Critic."""

    def __init__(
        self,
        base_model_path: str,
        value_head_path: str,
        strategies_path: str,
        lora_adapter_path: Optional[str] = None,
        pretrained_backbone_path: Optional[str] = None,
        device: str = None,
        use_chat_template: bool = True,
        cutoff_len: int = 2048,
        critic_reward_scale: float = 10.0
    ):
        """Initialize the evolved strategy selector.

        Args:
            base_model_path: Path to Qwen2.5-1.5B-Instruct base model (for structure)
            value_head_path: Path to trained value_head.bin
            strategies_path: Path to final_strategies_with_prompts.json
            lora_adapter_path: Optional path to LoRA adapter directory
            pretrained_backbone_path: Optional path to pretrained backbone weights
                (e.g., surrogate_full1/stage1/checkpoint-610/exported_bin/pytorch_model.bin)
            device: Device to use
            use_chat_template: Whether to use chat template
            cutoff_len: Maximum sequence length
            critic_reward_scale: Scale factor for Critic output (Critic outputs are scaled up by this factor during training)
        """
        self.critic_reward_scale = critic_reward_scale

        # Load Critic model
        self.critic = EvolvedCriticModel(
            base_model_path=base_model_path,
            value_head_path=value_head_path,
            lora_adapter_path=lora_adapter_path,
            pretrained_backbone_path=pretrained_backbone_path,
            device=device,
            use_chat_template=use_chat_template,
            cutoff_len=cutoff_len
        )

        # Load evolved strategies
        print(f"[EvolvedSelector] Loading strategies from {strategies_path}...")
        with open(strategies_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        self.strategies = data.get('strategies', [])
        print(f"[EvolvedSelector] Loaded {len(self.strategies)} evolved strategies")

        # Print top-3 strategies by training score
        print(f"[EvolvedSelector] Top-3 strategies by training score:")
        for i, s in enumerate(self.strategies[:3]):
            print(f"  {i+1}. {s.get('genotype_id', 'N/A')[:8]}... "
                  f"type={s.get('strategy_type', 'N/A')} "
                  f"total_score={s.get('scores', {}).get('total_score', 0):.4f}")

    def select_best_strategy(
        self,
        query: str,
        context: str,
        strategy_prompts: Dict[str, str] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """Select the best strategy based on Critic prediction.

 Critic SHORT_PROMPT strategy FULL_PROMPT

        Args:
            query: User query
            context: Content to optimize
            strategy_prompts: Ignored (for compatibility with old interface)

        Returns:
            Tuple of (best_strategy_id, result_dict)
            result_dict contains:
                - expected_reward: Critic predicted reward (scaled back to original range)
                - uncertainty: Always 0.0 (not predicted)
                - all_rankings: List of all strategies ranked by Critic
                - best_strategy: The full strategy dict (including FULL_PROMPT for Oracle)
        """
        # Rank all strategies using Critic with SHORT_PROMPT
        rankings = self.critic.rank_strategies(query, context, self.strategies)

        if not rankings:
            raise ValueError("No strategies available")

        # Get best strategy
        best_id, best_reward, best_strategy = rankings[0]

        # Scale reward back to original range (Critic outputs are scaled up)
        scaled_reward = best_reward / self.critic_reward_scale

        return best_id, {
            'expected_reward': scaled_reward,
            'uncertainty': 0.0,
            'all_rankings': [
                {
                    'strategy': gid,
                    'reward': reward / self.critic_reward_scale,
                    'uncertainty': 0.0,
                    'strategy_type': strategy.get('strategy_type', ''),
                    'full_prompt': strategy.get('full_prompt', ''),
                    'short_prompt': strategy.get('short_prompt', '')
                }
                for gid, reward, strategy in rankings
            ],
            'best_strategy': best_strategy
        }

    def get_strategy_by_id(self, genotype_id: str) -> Optional[Dict[str, Any]]:
        """Get strategy by genotype_id."""
        for s in self.strategies:
            if s.get('genotype_id') == genotype_id:
                return s
        return None


# Alias for compatibility
StrategySelector = EvolvedStrategySelector


def get_strategy_prompts() -> Dict[str, str]:
    """Get strategy prompts (for compatibility with old interface).

    Note: This is kept for API compatibility, but evolved strategies
    are loaded from the JSON file, not from this function.
    """
    return {}


def load_evolved_strategies(strategies_path: str) -> List[Dict[str, Any]]:
    """Load evolved strategies from JSON file.

    Args:
        strategies_path: Path to final_strategies_with_prompts.json

    Returns:
        List of strategy dicts
    """
    with open(strategies_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('strategies', [])
